{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46196519-7f88-40e8-a6ff-9fbe8a1182a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker.local import LocalSession\n",
    "from sagemaker import get_execution_role\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c41cbac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Windows Support for Local Mode is Experimental\n"
     ]
    }
   ],
   "source": [
    "session = LocalSession()\n",
    "session.config = {'local': {'local_code': True}}\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71b474d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model data ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a code\n",
      "a code/backbone.py\n",
      "a code/BEATs.py\n",
      "a code/inference.py\n",
      "a code/modules.py\n",
      "a code/quantizer.py\n",
      "a code/README.md\n",
      "a code/requirements.txt\n",
      "a code/Tokenizers.py\n",
      "a model.pt\n"
     ]
    }
   ],
   "source": [
    "!tar -czvf model.tar.gz code model.pt\n",
    "print(\"Model data ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0b9d923",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"model.pt\"\n",
    "model_data = \"model.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e91cf24-ab72-482d-aec7-42f912c5fab6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = PyTorchModel(entry_point = \"inference.py\",\n",
    "py_version=\"py310\",\n",
    "framework_version=\"2.0\",\n",
    "model_data = model_data,\n",
    "env = {\"MODEL_NAME\": model_name},\n",
    "role = role,\n",
    "sagemaker_session = session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4136affb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the short-lived AWS credentials found in session. They might expire while running.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attaching to 20sm6k14dm-algo-1-r26to\n",
      "20sm6k14dm-algo-1-r26to  | Requirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (from -r /opt/ml/model/code/requirements.txt (line 1)) (1.11.0+cpu)\n",
      "20sm6k14dm-algo-1-r26to  | Requirement already satisfied: torchaudio in /opt/conda/lib/python3.8/site-packages (from -r /opt/ml/model/code/requirements.txt (line 2)) (0.11.0+cpu)\n",
      "20sm6k14dm-algo-1-r26to  | Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from -r /opt/ml/model/code/requirements.txt (line 3)) (1.24.2)\n",
      "20sm6k14dm-algo-1-r26to  | Requirement already satisfied: boto3 in /opt/conda/lib/python3.8/site-packages (from -r /opt/ml/model/code/requirements.txt (line 4)) (1.26.82)\n",
      "20sm6k14dm-algo-1-r26to  | Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch->-r /opt/ml/model/code/requirements.txt (line 1)) (4.5.0)\n",
      "20sm6k14dm-algo-1-r26to  | Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from boto3->-r /opt/ml/model/code/requirements.txt (line 4)) (1.0.1)\n",
      "20sm6k14dm-algo-1-r26to  | Requirement already satisfied: botocore<1.30.0,>=1.29.82 in /opt/conda/lib/python3.8/site-packages (from boto3->-r /opt/ml/model/code/requirements.txt (line 4)) (1.29.82)\n",
      "20sm6k14dm-algo-1-r26to  | Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from boto3->-r /opt/ml/model/code/requirements.txt (line 4)) (0.6.0)\n",
      "20sm6k14dm-algo-1-r26to  | Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.8/site-packages (from botocore<1.30.0,>=1.29.82->boto3->-r /opt/ml/model/code/requirements.txt (line 4)) (2.8.2)\n",
      "20sm6k14dm-algo-1-r26to  | Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.8/site-packages (from botocore<1.30.0,>=1.29.82->boto3->-r /opt/ml/model/code/requirements.txt (line 4)) (1.26.14)\n",
      "20sm6k14dm-algo-1-r26to  | Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.82->boto3->-r /opt/ml/model/code/requirements.txt (line 4)) (1.16.0)\n",
      "20sm6k14dm-algo-1-r26to  | \u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "20sm6k14dm-algo-1-r26to  | \u001b[0m\n",
      "20sm6k14dm-algo-1-r26to  | \n",
      "20sm6k14dm-algo-1-r26to  | \u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "20sm6k14dm-algo-1-r26to  | \u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "20sm6k14dm-algo-1-r26to  | ['torchserve', '--start', '--model-store', '/.sagemaker/ts/models', '--ts-config', '/etc/sagemaker-ts.properties', '--log-config', '/opt/conda/lib/python3.8/site-packages/sagemaker_pytorch_serving_container/etc/log4j2.xml', '--models', 'model=/opt/ml/model']\n",
      "20sm6k14dm-algo-1-r26to  | WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:36,418 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:36,637 [INFO ] main org.pytorch.serve.ModelServer - \n",
      "20sm6k14dm-algo-1-r26to  | Torchserve version: 0.6.0\n",
      "20sm6k14dm-algo-1-r26to  | TS Home: /opt/conda/lib/python3.8/site-packages\n",
      "20sm6k14dm-algo-1-r26to  | Current directory: /\n",
      "20sm6k14dm-algo-1-r26to  | Temp directory: /home/model-server/tmp\n",
      "20sm6k14dm-algo-1-r26to  | Number of GPUs: 0\n",
      "20sm6k14dm-algo-1-r26to  | Number of CPUs: 12\n",
      "20sm6k14dm-algo-1-r26to  | Max heap size: 1954 M\n",
      "20sm6k14dm-algo-1-r26to  | Python executable: /opt/conda/bin/python3.8\n",
      "20sm6k14dm-algo-1-r26to  | Config file: /etc/sagemaker-ts.properties\n",
      "20sm6k14dm-algo-1-r26to  | Inference address: http://0.0.0.0:8080\n",
      "20sm6k14dm-algo-1-r26to  | Management address: http://0.0.0.0:8080\n",
      "20sm6k14dm-algo-1-r26to  | Metrics address: http://127.0.0.1:8082\n",
      "20sm6k14dm-algo-1-r26to  | Model Store: /.sagemaker/ts/models\n",
      "20sm6k14dm-algo-1-r26to  | Initial Models: model=/opt/ml/model\n",
      "20sm6k14dm-algo-1-r26to  | Log dir: /logs\n",
      "20sm6k14dm-algo-1-r26to  | Metrics dir: /logs\n",
      "20sm6k14dm-algo-1-r26to  | Netty threads: 0\n",
      "20sm6k14dm-algo-1-r26to  | Netty client threads: 0\n",
      "20sm6k14dm-algo-1-r26to  | Default workers per model: 12\n",
      "20sm6k14dm-algo-1-r26to  | Blacklist Regex: N/A\n",
      "20sm6k14dm-algo-1-r26to  | Maximum Response Size: 6553500\n",
      "20sm6k14dm-algo-1-r26to  | Maximum Request Size: 6553500\n",
      "20sm6k14dm-algo-1-r26to  | Limit Maximum Image Pixels: true\n",
      "20sm6k14dm-algo-1-r26to  | Prefer direct buffer: false\n",
      "20sm6k14dm-algo-1-r26to  | Allowed Urls: [file://.*|http(s)?://.*]\n",
      "20sm6k14dm-algo-1-r26to  | Custom python dependency for model allowed: false\n",
      "20sm6k14dm-algo-1-r26to  | Metrics report format: prometheus\n",
      "20sm6k14dm-algo-1-r26to  | Enable metrics API: true\n",
      "20sm6k14dm-algo-1-r26to  | Workflow Store: /.sagemaker/ts/models\n",
      "20sm6k14dm-algo-1-r26to  | Model config: N/A\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:36,648 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:36,678 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: /opt/ml/model\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:36,684 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive version is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:36,684 [WARN ] main org.pytorch.serve.archive.model.ModelArchive - Model archive createdOn is not defined. Please upgrade to torch-model-archiver 0.2.0 or higher\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:36,687 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model model loaded.\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:36,714 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:36,912 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://0.0.0.0:8080\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:36,914 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:36,919 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8082\n",
      "20sm6k14dm-algo-1-r26to  | Model server started.\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:37,466 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:38,004 [INFO ] pool-2-thread-13 ACCESS_LOG - /172.20.0.1:46484 \"GET /ping HTTP/1.1\" 200 140\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:38,005 [INFO ] pool-2-thread-13 TS_METRICS - Requests2XX.Count:1|#Level:Host|#hostname:509bea7a7a27,timestamp:1693862377\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:38,206 [INFO ] pool-3-thread-1 TS_METRICS - CPUUtilization.Percent:92.3|#Level:Host|#hostname:509bea7a7a27,timestamp:1693862378\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:38,207 [INFO ] pool-3-thread-1 TS_METRICS - DiskAvailable.Gigabytes:921.970516204834|#Level:Host|#hostname:509bea7a7a27,timestamp:1693862378\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:38,208 [INFO ] pool-3-thread-1 TS_METRICS - DiskUsage.Gigabytes:33.66779327392578|#Level:Host|#hostname:509bea7a7a27,timestamp:1693862378\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:38,208 [INFO ] pool-3-thread-1 TS_METRICS - DiskUtilization.Percent:3.5|#Level:Host|#hostname:509bea7a7a27,timestamp:1693862378\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:38,208 [INFO ] pool-3-thread-1 TS_METRICS - MemoryAvailable.Megabytes:5810.03125|#Level:Host|#hostname:509bea7a7a27,timestamp:1693862378\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:38,209 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUsed.Megabytes:1704.79296875|#Level:Host|#hostname:509bea7a7a27,timestamp:1693862378\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:38,209 [INFO ] pool-3-thread-1 TS_METRICS - MemoryUtilization.Percent:25.6|#Level:Host|#hostname:509bea7a7a27,timestamp:1693862378\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,218 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9007\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,223 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - [PID]77\n",
      "20sm6k14dm-algo-1-r26to  | \n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,224 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Torch worker started.\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,224 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,232 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9007\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,251 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9002\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,253 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - [PID]80\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,254 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Torch worker started.\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,255 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9002\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,256 [INFO ] W-9008-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9008\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,258 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,263 [INFO ] W-9008-model_1.0-stdout MODEL_LOG - [PID]76\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,265 [INFO ] W-9008-model_1.0-stdout MODEL_LOG - Torch worker started.\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,265 [INFO ] W-9008-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,265 [INFO ] W-9008-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9008\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,273 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9007.\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,293 [INFO ] W-9007-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1693862379293\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,307 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9002.\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,308 [INFO ] W-9008-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1693862379308\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,307 [INFO ] W-9008-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9008.\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,327 [INFO ] W-9002-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1693862379327\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,351 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9001\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,358 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - [PID]83\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,365 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Torch worker started.\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,366 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,365 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9001\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,384 [INFO ] W-9011-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9011\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,385 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9003\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,386 [INFO ] W-9011-model_1.0-stdout MODEL_LOG - [PID]81\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,387 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - [PID]79\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,387 [INFO ] W-9011-model_1.0-stdout MODEL_LOG - Torch worker started.\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,387 [INFO ] W-9011-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,387 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Torch worker started.\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,387 [INFO ] W-9011-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9011\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,388 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9003\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,388 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,445 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9004\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,451 [INFO ] W-9008-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,452 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - [PID]78\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,452 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Torch worker started.\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,453 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,453 [INFO ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9004\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,453 [INFO ] W-9003-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1693862379453\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,456 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,457 [INFO ] W-9001-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1693862379456\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,453 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9003.\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,446 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9000\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,449 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,459 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - [PID]82\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,448 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9001.\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,459 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Torch worker started.\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,489 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,490 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9000\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,496 [INFO ] W-9011-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9011.\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,496 [INFO ] W-9011-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1693862379496\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,581 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9006\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,582 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - [PID]110\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,583 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Torch worker started.\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,583 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,584 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9006\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,585 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,602 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,613 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9004.\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,658 [INFO ] W-9004-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1693862379658\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,661 [INFO ] W-9000-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1693862379661\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,671 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9006.\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,671 [INFO ] W-9011-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,670 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9000.\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,675 [INFO ] W-9006-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1693862379675\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,700 [INFO ] W-9009-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9009\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,786 [INFO ] W-9009-model_1.0-stdout MODEL_LOG - [PID]108\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,787 [INFO ] W-9009-model_1.0-stdout MODEL_LOG - Torch worker started.\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,796 [INFO ] W-9009-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,777 [INFO ] W-9010-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9010\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,799 [INFO ] W-9010-model_1.0-stdout MODEL_LOG - [PID]109\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,800 [INFO ] W-9010-model_1.0-stdout MODEL_LOG - Torch worker started.\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,788 [INFO ] W-9009-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9009\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,802 [INFO ] W-9010-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9010\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,839 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,840 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,857 [INFO ] W-9010-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,873 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,881 [INFO ] W-9010-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9010.\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,881 [INFO ] W-9010-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1693862379881\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,911 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Listening on port: /home/model-server/tmp/.ts.sock.9005\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,914 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - [PID]111\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,916 [INFO ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.ts.sock.9005\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,922 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Torch worker started.\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,923 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Python runtime: 3.8.13\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,960 [INFO ] W-9010-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,973 [INFO ] W-9009-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9009.\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,974 [INFO ] W-9009-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1693862379974\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:39,978 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - Connection accepted: /home/model-server/tmp/.ts.sock.9005.\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:40,012 [INFO ] W-9005-model_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req. to backend at: 1693862380011\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:40,034 [INFO ] W-9009-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:40,054 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - model_name: model, batchSize: 1\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:47,188 [INFO ] W-9001-model_1.0-stdout MODEL_LOG - BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 1.0, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': False, 'predictor_dropout': 0.1, 'predictor_class': 527}\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:47,188 [INFO ] W-9004-model_1.0-stdout MODEL_LOG - BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 1.0, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': False, 'predictor_dropout': 0.1, 'predictor_class': 527}\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:47,190 [INFO ] W-9002-model_1.0-stdout MODEL_LOG - BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 1.0, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': False, 'predictor_dropout': 0.1, 'predictor_class': 527}\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:47,194 [INFO ] W-9006-model_1.0-stdout MODEL_LOG - BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 1.0, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': False, 'predictor_dropout': 0.1, 'predictor_class': 527}\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:47,195 [INFO ] W-9011-model_1.0-stdout MODEL_LOG - BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 1.0, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': False, 'predictor_dropout': 0.1, 'predictor_class': 527}\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:47,197 [INFO ] W-9000-model_1.0-stdout MODEL_LOG - BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 1.0, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': False, 'predictor_dropout': 0.1, 'predictor_class': 527}\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:47,199 [INFO ] W-9007-model_1.0-stdout MODEL_LOG - BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 1.0, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': False, 'predictor_dropout': 0.1, 'predictor_class': 527}\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:47,202 [INFO ] W-9005-model_1.0-stdout MODEL_LOG - BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 1.0, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': False, 'predictor_dropout': 0.1, 'predictor_class': 527}\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:47,203 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 1.0, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': False, 'predictor_dropout': 0.1, 'predictor_class': 527}\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:47,205 [INFO ] W-9010-model_1.0-stdout MODEL_LOG - BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 1.0, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': False, 'predictor_dropout': 0.1, 'predictor_class': 527}\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:47,227 [INFO ] W-9008-model_1.0-stdout MODEL_LOG - BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 1.0, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': False, 'predictor_dropout': 0.1, 'predictor_class': 527}\n",
      "20sm6k14dm-algo-1-r26to  | 2023-09-04T17:19:47,233 [INFO ] W-9009-model_1.0-stdout MODEL_LOG - BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 1.0, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': False, 'predictor_dropout': 0.1, 'predictor_class': 527}\n",
      "!"
     ]
    }
   ],
   "source": [
    "predictor = model.deploy(\n",
    "    initial_instance_count = 1,\n",
    "    instance_type = 'local',\n",
    "    # content_type = 'application/octet-stream'\n",
    "    content_type = 'audio/wav'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c416c4db",
   "metadata": {},
   "outputs": [
    {
     "ename": "ProtocolError",
     "evalue": "('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteDisconnected\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\u633714\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 714\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_request(\n\u001b[0;32m    715\u001b[0m     conn,\n\u001b[0;32m    716\u001b[0m     method,\n\u001b[0;32m    717\u001b[0m     url,\n\u001b[0;32m    718\u001b[0m     timeout\u001b[39m=\u001b[39mtimeout_obj,\n\u001b[0;32m    719\u001b[0m     body\u001b[39m=\u001b[39mbody,\n\u001b[0;32m    720\u001b[0m     headers\u001b[39m=\u001b[39mheaders,\n\u001b[0;32m    721\u001b[0m     chunked\u001b[39m=\u001b[39mchunked,\n\u001b[0;32m    722\u001b[0m )\n\u001b[0;32m    724\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\u633714\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    462\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    463\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    464\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    465\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m--> 466\u001b[0m             six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    467\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\u633714\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 461\u001b[0m     httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[0;32m    462\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    463\u001b[0m     \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    464\u001b[0m     \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    465\u001b[0m     \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\u633714\\AppData\\Local\\anaconda3\\Lib\\http\\client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1377\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1378\u001b[0m     response\u001b[39m.\u001b[39mbegin()\n\u001b[0;32m   1379\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\u633714\\AppData\\Local\\anaconda3\\Lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_read_status()\n\u001b[0;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n",
      "File \u001b[1;32mc:\\Users\\u633714\\AppData\\Local\\anaconda3\\Lib\\http\\client.py:287\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m line:\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Presumably, the server closed the connection before\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# sending a valid response.\u001b[39;00m\n\u001b[1;32m--> 287\u001b[0m     \u001b[39mraise\u001b[39;00m RemoteDisconnected(\u001b[39m\"\u001b[39m\u001b[39mRemote end closed connection without\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    288\u001b[0m                              \u001b[39m\"\u001b[39m\u001b[39m response\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    289\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;31mRemoteDisconnected\u001b[0m: Remote end closed connection without response",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m     audio_bytestream \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mread()\n\u001b[0;32m      5\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m----> 6\u001b[0m predictions \u001b[39m=\u001b[39m predictor\u001b[39m.\u001b[39mpredict(audio_bytestream)\n\u001b[0;32m      7\u001b[0m duration \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n\u001b[0;32m      9\u001b[0m \u001b[39mprint\u001b[39m(predictions)\n",
      "File \u001b[1;32mc:\\Users\\u633714\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sagemaker\\base_predictor.py:185\u001b[0m, in \u001b[0;36mPredictor.predict\u001b[1;34m(self, data, initial_args, target_model, target_variant, inference_id, custom_attributes)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return the inference from the specified endpoint.\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \n\u001b[0;32m    140\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[39m        as is.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    177\u001b[0m request_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_request_args(\n\u001b[0;32m    178\u001b[0m     data,\n\u001b[0;32m    179\u001b[0m     initial_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    183\u001b[0m     custom_attributes,\n\u001b[0;32m    184\u001b[0m )\n\u001b[1;32m--> 185\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msagemaker_session\u001b[39m.\u001b[39msagemaker_runtime_client\u001b[39m.\u001b[39minvoke_endpoint(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mrequest_args)\n\u001b[0;32m    186\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_response(response)\n",
      "File \u001b[1;32mc:\\Users\\u633714\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sagemaker\\local\\local_session.py:597\u001b[0m, in \u001b[0;36mLocalSagemakerRuntimeClient.invoke_endpoint\u001b[1;34m(self, Body, EndpointName, ContentType, Accept, CustomAttributes, TargetModel, TargetVariant, InferenceId)\u001b[0m\n\u001b[0;32m    595\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(Body, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    596\u001b[0m     Body \u001b[39m=\u001b[39m Body\u001b[39m.\u001b[39mencode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 597\u001b[0m r \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhttp\u001b[39m.\u001b[39mrequest(\u001b[39m\"\u001b[39m\u001b[39mPOST\u001b[39m\u001b[39m\"\u001b[39m, url, body\u001b[39m=\u001b[39mBody, preload_content\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, headers\u001b[39m=\u001b[39mheaders)\n\u001b[0;32m    599\u001b[0m \u001b[39mreturn\u001b[39;00m {\u001b[39m\"\u001b[39m\u001b[39mBody\u001b[39m\u001b[39m\"\u001b[39m: r, \u001b[39m\"\u001b[39m\u001b[39mContentType\u001b[39m\u001b[39m\"\u001b[39m: Accept}\n",
      "File \u001b[1;32mc:\\Users\\u633714\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\request.py:78\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[1;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_encode_url(\n\u001b[0;32m     75\u001b[0m         method, url, fields\u001b[39m=\u001b[39mfields, headers\u001b[39m=\u001b[39mheaders, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39murlopen_kw\n\u001b[0;32m     76\u001b[0m     )\n\u001b[0;32m     77\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_encode_body(\n\u001b[0;32m     79\u001b[0m         method, url, fields\u001b[39m=\u001b[39mfields, headers\u001b[39m=\u001b[39mheaders, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39murlopen_kw\n\u001b[0;32m     80\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\u633714\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\request.py:170\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[1;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m extra_kw[\u001b[39m\"\u001b[39m\u001b[39mheaders\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mupdate(headers)\n\u001b[0;32m    168\u001b[0m extra_kw\u001b[39m.\u001b[39mupdate(urlopen_kw)\n\u001b[1;32m--> 170\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39murlopen(method, url, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mextra_kw)\n",
      "File \u001b[1;32mc:\\Users\\u633714\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\poolmanager.py:376\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[1;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[0;32m    374\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39murlopen(method, url, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    375\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 376\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39murlopen(method, u\u001b[39m.\u001b[39mrequest_uri, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    378\u001b[0m redirect_location \u001b[39m=\u001b[39m redirect \u001b[39mand\u001b[39;00m response\u001b[39m.\u001b[39mget_redirect_location()\n\u001b[0;32m    379\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[1;32mc:\\Users\\u633714\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:798\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    795\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, (SocketError, HTTPException)):\n\u001b[0;32m    796\u001b[0m     e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[1;32m--> 798\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39mincrement(\n\u001b[0;32m    799\u001b[0m     method, url, error\u001b[39m=\u001b[39me, _pool\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, _stacktrace\u001b[39m=\u001b[39msys\u001b[39m.\u001b[39mexc_info()[\u001b[39m2\u001b[39m]\n\u001b[0;32m    800\u001b[0m )\n\u001b[0;32m    801\u001b[0m retries\u001b[39m.\u001b[39msleep()\n\u001b[0;32m    803\u001b[0m \u001b[39m# Keep track of the error for the retry warning.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\u633714\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\util\\retry.py:550\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    547\u001b[0m \u001b[39melif\u001b[39;00m error \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_read_error(error):\n\u001b[0;32m    548\u001b[0m     \u001b[39m# Read retry?\u001b[39;00m\n\u001b[0;32m    549\u001b[0m     \u001b[39mif\u001b[39;00m read \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_method_retryable(method):\n\u001b[1;32m--> 550\u001b[0m         \u001b[39mraise\u001b[39;00m six\u001b[39m.\u001b[39mreraise(\u001b[39mtype\u001b[39m(error), error, _stacktrace)\n\u001b[0;32m    551\u001b[0m     \u001b[39melif\u001b[39;00m read \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    552\u001b[0m         read \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\u633714\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\packages\\six.py:769\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    767\u001b[0m         value \u001b[39m=\u001b[39m tp()\n\u001b[0;32m    768\u001b[0m     \u001b[39mif\u001b[39;00m value\u001b[39m.\u001b[39m__traceback__ \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m tb:\n\u001b[1;32m--> 769\u001b[0m         \u001b[39mraise\u001b[39;00m value\u001b[39m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m    770\u001b[0m     \u001b[39mraise\u001b[39;00m value\n\u001b[0;32m    771\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\u633714\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    713\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 714\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_request(\n\u001b[0;32m    715\u001b[0m     conn,\n\u001b[0;32m    716\u001b[0m     method,\n\u001b[0;32m    717\u001b[0m     url,\n\u001b[0;32m    718\u001b[0m     timeout\u001b[39m=\u001b[39mtimeout_obj,\n\u001b[0;32m    719\u001b[0m     body\u001b[39m=\u001b[39mbody,\n\u001b[0;32m    720\u001b[0m     headers\u001b[39m=\u001b[39mheaders,\n\u001b[0;32m    721\u001b[0m     chunked\u001b[39m=\u001b[39mchunked,\n\u001b[0;32m    722\u001b[0m )\n\u001b[0;32m    724\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[0;32m    728\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\u633714\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    461\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[0;32m    462\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    463\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    464\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    465\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m--> 466\u001b[0m             six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    467\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    468\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\u633714\\AppData\\Local\\anaconda3\\Lib\\site-packages\\urllib3\\connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    459\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[0;32m    460\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 461\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[0;32m    462\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    463\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    464\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    465\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m    466\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\u633714\\AppData\\Local\\anaconda3\\Lib\\http\\client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1376\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1377\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1378\u001b[0m         response\u001b[39m.\u001b[39mbegin()\n\u001b[0;32m   1379\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[0;32m   1380\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\u633714\\AppData\\Local\\anaconda3\\Lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_read_status()\n\u001b[0;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[0;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\u633714\\AppData\\Local\\anaconda3\\Lib\\http\\client.py:287\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    283\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mreply:\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mrepr\u001b[39m(line))\n\u001b[0;32m    284\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m line:\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Presumably, the server closed the connection before\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# sending a valid response.\u001b[39;00m\n\u001b[1;32m--> 287\u001b[0m     \u001b[39mraise\u001b[39;00m RemoteDisconnected(\u001b[39m\"\u001b[39m\u001b[39mRemote end closed connection without\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    288\u001b[0m                              \u001b[39m\"\u001b[39m\u001b[39m response\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    289\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    290\u001b[0m     version, status, reason \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39msplit(\u001b[39mNone\u001b[39;00m, \u001b[39m2\u001b[39m)\n",
      "\u001b[1;31mProtocolError\u001b[0m: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))"
     ]
    }
   ],
   "source": [
    "audio_file = 'baby.wav'\n",
    "with open(audio_file, 'rb') as f:\n",
    "    audio_bytestream = f.read()\n",
    "\n",
    "start_time = time.time()\n",
    "predictions = predictor.predict(audio_bytestream)\n",
    "duration = time.time() - start_time\n",
    "\n",
    "print(predictions)\n",
    "print(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f558c1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
